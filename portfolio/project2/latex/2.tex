% vim: set spell:

\section{}

\subsection*{Tools}

For this exercise it was chosen to use Lemur 4.12 out of considerations for the
reader. The reader has, if nothing else, access to Lab session 2 documentation,
explaining how to install and run Lemur 4.12 together with a Python interface
for Lemur. The reader is thereby made more capable of reproducing the results
below.

The only notational difference between the examples below and the Lab session 2
documentation, is that a Python script by the name \lstinline$x$ is executed as
\lstinline$python2 x$ instead of \lstinline$./x$, as the exercise was completed
on a system with multiple Python installations. Specifically, Python 2.7.6 was
used.

\subsection*{Dataset}

The handed out zip file\footnotemark~containing the Cystic Fibrosis data
collection has

\footnotetext{Downloaded from
\lstinline$http://itlab.dbit.dk/~toine/uchfall2010/test-collection.zip$.}

\begin{lstlisting}
$ md5sum test-collection.zip 
0eca42ff1f4cbba1129a13ef743cda54  test-collection.zip
\end{lstlisting}

The handed out Python interface for Lemur\footnotemark~(containing a stopword
list) has

\footnotetext{Downloaded from
\lstinline|http://itlab.dbit.dk/~toine/uchfall2010/lemur-interface.zip|.}

\begin{lstlisting}
$ md5sum lemur-interface.zip 
e32028007c903a8639c1a01a23c954cd  lemur-interface.zip
\end{lstlisting}

We assume that the handed out data set is complete. That is, no new documents
are meant to be added to the system after indexing. In the converse case, it
would have been more appropriate to use cross-validation in the tasks below.

Let \lstinline|$TYPE| be either \lstinline$title$, \lstinline$abstract$,
\lstinline$subjects$, or \lstinline$all$.

The data was already split into the different fields, stored in different files
with the names \lstinline|cystic-fibrosis.$TYPE.sgml|. We'll rename them to
simply \lstinline|$TYPE.sgml|.

The data is mixed case. Lemur seems to perform case insensitive indexing and
searching, but this has not been confirmed in any other way, as the Lemur
documentation is seemingly rather lacking. Attempting to upper case all the
data for prior to some of the below runs, did not yield better results.

Tokenization and custom stopword filtering has been attempted, but this seemed
to have little gain over simply using the original dataset. The attempts are
documented in the appendices. Not stemming (e.g. the subjects dataset) did not
yield better results either.

In the end, tokenization was not performed.  Stopword filtering was done
against the comperehensive stopword list found together with the Python
interface for Lemur. For stemming, the Krovetz stemmer was used.

The handed out relevance judgements score each document as either marginally
relevant (1) or highly relevant (2), leaving out irrelevant documents. Only
nDCG deals with such graded relevance assessments, so if we chose any other
metric, we would have to discard some relevant information already at this
stage.

To simplify the running of tests, we devise a shell script (named e.g.
\lstinline|worthog|).

\lstinputlisting{../../../lab/2/warthdog}

It seems unreliable to choose a ranking model based on one choice of field
only. The dataset is sufficiently small, for us to consider many possible
permuations. The results varying the choice of field and ranking model are
shown in \referToTable{ranking-model-vs-fields}.

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\lstinline|$R_MODEL| & \lstinline$title$ & \lstinline$abstract$ &
    \lstinline$subjects$ & \lstinline$all$ \\
\hline
\lstinline$-m 1$ & $0.4075$ & $0.4380$ & $0.4080$ & $\mathbf{0.5922}$ \\
\hline
\lstinline$-m 4 DirchletPrior=3000$ & $0.3956$ & $0.4109$ & $0.4001$ & $\mathbf{0.5721}$ \\
\lstinline$-m 4 DirchletPrior=2000$ & $0.3971$ & $0.4160$ & $0.4032$ & $\mathbf{0.5750}$ \\
\lstinline$-m 4 DirchletPrior=300$ & $0.4052$ & $\underline{0.4242}$ & $\underline{0.4090}$ & $\mathbf{0.5887}$ \\
\lstinline$-m 4 DirchletPrior=200$ & $\underline{0.4074}$ & $0.4202$ & $0.4080$ & $\underline{\mathbf{0.5894}}$ \\
\lstinline$-m 4 DirchletPrior=100$ & $0.4061$ & $0.4148$ & $0.4070$ & $\mathbf{0.5871}$ \\
\hline
\lstinline$-m 3 JelinekMercerLambda=0.95$ & $0.3944$ & $0.4130$ & $0.3922$ & $\mathbf{0.5796}$ \\
\lstinline$-m 3 JelinekMercerLambda=0.9$ & $\underline{0.3989}$ & $0.4152$ & $0.3957$ &
    $\underline{\mathbf{0.5882}}$ \\
\lstinline$-m 3 JelinekMercerLambda=0.8$ & $0.3958$ & $\underline{0.4171}$ & $0.4003$ & $\mathbf{0.5870}$ \\
\lstinline$-m 3 JelinekMercerLambda=0.7$ & $0.3964$ & $0.4156$ & $\underline{0.4025}$ & $\mathbf{0.5854}$ \\
\lstinline$-m 3 JelinekMercerLambda=0.1$ & $0.3798$ & $0.3989$ & $0.4022$ & $\mathbf{0.5494}$ \\
\hline
\end{tabular}
\caption[]{Choice of ranking model vs. choice of fields, nDCG. The data was
filtered with the handed out stopwords and stemmed using the Krovetz stemmer.
Including all the fields yields the best results for every row (bold).  For
every column the underlined cell indicates the seemingly best choice of
parameter for a particular method. 1 - Vector Space model with TFIDF weighting,
4 - Language modeling with Dirchlet smooting, 3 - Language modeling with
Jelinek-Mercer smoothing.}
\label{table:ranking-model-vs-fields}
\end{table}

The results indicate that the Vector Space model with TFIDF weighting (and
default Lemur 4.12 parameters) slightly outperforms all other methods (with
varying parameters). Also, choosing to index using all the fields seems to
yield the best results.

\referToTable{evaluation-measure-vs-fields} compares the nDCG, MAP and MRR
evaluation measures.  The table also includes precision and recall measures for
the purposes of discussion.

Mean Reciprocal Rank (MRR) considers only how many results the user has to skip
from the top before reaching a relevant result. With the exception of using
only the subjects field for indexing, the user can expect to see the a relevant
result as the very first result.

Mean Average Precision (MAP) gives higher weights to relevant results high up
in the results list, while giving 0 weights to noise. Summing over the result
list and dividing the by the number of results yields a MAP. The MAP results in
our case indicate that either there are few results high up in the result set,
or that there is a lot of noise in the result set.

This is also what we see if we look at our precision, a lot of the results
returned are highly irrelevant. Our recall tells us again that indexing using
all fields gives us most relevant results upon querying.

Normalized Discounted Cumulative Gain (nDCG), allowes graded relevance rather
than binary. It gives preference to most relevant results ahead of the list.
This was chosen as the primary measure as the original relevance judgements
were graded. Having considered other evaluation measures, nDCG does seem to
provide the most fair judgement for our information retrieval problem.

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\lstinline|$E_MEASURE| & \lstinline$title$ & \lstinline$abstract$ &
    \lstinline$subjects$ & \lstinline$all$ \\
\hline
\lstinline$-m ndcg$ & $0.4075$ & $0.4380$ & $0.4080$ & $\mathbf{0.5922}$ \\
\lstinline$-m map$ & $0.1858$ & $0.1969$ & $0.1963$ & $\textbf{0.3003}$ \\
\lstinline$-m recip_rank$ & $0.6179$ & $\mathbf{0.7558}$ & $0.4580$ & $0.7361$ \\
(precision) & $\approx 0.05$ & $\approx 0.02$ & $\mathbf{\approx 0.08}$ & $\approx 0.03$ \\
(recall) & $\approx 0.42$ & $\approx 0.51$ & $\approx 0.56$ & $\mathbf{\approx 0.79}$ \\
\hline
\end{tabular}
\caption[]{Choice of evaluation measure vs. choice of fields. All results use
the Vector Space with TFIDF weighting model. The data was filtered with the
handed out stopwords and stemmed using the Krovetz stemmer.  Including all the
fields yields the best results for (almost) every row. The best results for
each evaluation measure are marked in bold.}
\label{table:evaluation-measure-vs-fields}
\end{table}

