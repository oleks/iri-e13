% vim: set spell:

\section{}

\label{appendix:1}

The answer to this exercise is presented in such a manner that the reader can
reproduce the results on a *nix system, e.g. an Ubuntu. Feel welcome to open a
terminal and copy the below commands as you go along. It is assumed that the
reader is familiar with Unix pipes and standard output redirection, i.e. the
operators \texttt{|} and \texttt{>}.

We analyze the handed out query log. This is a file named
\texttt{excite-999000.txt} obtained from the course web site on November 18.

To be more confident that we're working with the same file, we can compare our
\texttt{md5sum}s:

\begin{lstlisting}
$ md5sum excite-999000.txt
514321481e3b2347afe25d0243c88606  excite-999000.txt
\end{lstlisting}

Since the 3rd column is to be ignored, we start by placing all but the 3rd
column (separated by tabs) into a new file, \texttt{excite-999000.txt.no3}:

\begin{lstlisting}
$ cat excite-999000.txt | cut -f1,2,4- - > excite-999000.no3.txt
\end{lstlisting}

If we look at the number of lines, it appears that we have a log of 999000
queries:

\begin{lstlisting}
$ wc -l excite-999000.no3.txt
999000 excite-999000.no3.txt
\end{lstlisting}

Before further processing, we'll sort the queries by hashed user id and then
timestamp, i.e. first by column 2, then column 1.

\begin{lstlisting}
$ sort -k2,2 -k1,1 excite-999000.no3.txt > excite-999000.no3.by.2-1.txt
\end{lstlisting}

Eliminating duplicates, we see that we really 996299 queries. This
normalization is justifiable as the same user sending the same query within the
time frame of one second is most probably due to a technical glitch.

\begin{lstlisting}
$ uniq excite-999000.no3.by.2-1.txt > excite-999000.no3.by.2-1.uniq.txt
$ wc -l excite-999000.no3.by.2-1.uniq.txt 
996299 excite-999000.no3.by.2-1.uniq.txt
\end{lstlisting}

\begin{itemize}

\item \emph{What is the mean number of queries per user id?}

We assume that no pair of user ids hash to the same ``hashed user id'',
so a unique hash designates a unique user id.

To compute the mean number of queries per user id, we pick out the user id
column, and count how many times a user id appears in consecutive order. This
gives a count the number of queries issued by each user id.

\begin{lstlisting}
$ cut -f2 excite-999000.no3.by.2-1.uniq.txt | uniq -c | \
  sort -nr > excite-999000.no3.query.counts.txt
$ head -5 excite-999000.no3.query.counts.txt 
  52883 F54BC573FF3C94B1
  11999 02F1CA1699E5D828
   6880 245832BC3425778D
   6483 18E21BC7D100C75A
   6404 E43DD6D82BFBD0B8
\end{lstlisting}

We now extract the first column (the counts), and use \texttt{awk} utility to
compute the median and standard deviation.

\begin{lstlisting}
$ sed "s/ *\([0-9]\+\) .*/\1/" excite-999000.no3.query.counts.txt | awk \
  "{x[NR]=$0; s+=$0} END{md=s/NR; \
   for (i in x){ss += (x[i]-md)^2} sd = sqrt(ss/NR); \
   print "MD = "md; print "SD = "sd}"
MD = 4.297
SD = 135.462
\end{lstlisting}

So the number of queries per user is roughly $4.3\pm135.5$, meaning that
roughly 68\% of the users perform between $1$ and $140$ queries.

\item \emph{Analyse the variability of query length (i.e., in words or in
characters).}

We extract the third column (queries), and sort them in lexicographic order:

\begin{lstlisting}
$ cut -f3- excite-999000.no3.by.2-1.uniq.txt | sort > queries.txt
\end{lstlisting}

We'll use an approach similar to the above to find the median and standard
deviation of the query length (in characters).

\begin{lstlisting}
$ cat queries.txt | awk "{x[NR]=length($0); s+=length($0)} \
  END{md=s/NR; for (i in x){ss += (x[i]-md)^2} sd = sqrt(ss/NR); \
  print "MD = "md; print "SD = "sd}"
MD = 20.4887
SD = 14.365
\end{lstlisting}

So a query is roughly $20.5\pm 14.4$ characters long, meaning that roguhly 68\%
of the queries have a character length between $6.1$ and $34.9$.

The reason that we choose to do this based on characters is because by looking
at the tail of the \texttt{queries.txt} file, we find that a few queries are
written in character-based languages. 

\item \emph{What percentage of queries are mixed case? Upper case? Lower case?}

We assume that this question asks to analyse the casing of all queries, not
just unique queries. The latter is easily obtainable by first sorting and
extracting unique queries, but otherwise following the method below.

First, we remove all non-alphabetic (in terms of ASCII) characters from the
queries, and ignore all those queries which are now empty texts:

\begin{lstlisting}
$ sed "s/[^a-zA-Z]//g" queries.txt | sed "/^$/d" > queries.alpha.txt
\end{lstlisting}

Note, \texttt{queries.alpha.txt} omits those queries that have no alphabetic
characters at all. It is assumed that it is non-sensical to speak about the
casing of e.g. numeric queries.

First, we'll count the number of queries we're now dealing with:

\begin{lstlisting}
$ wc -l queries.alpha.txt 
995241 queries.alpha.txt
\end{lstlisting}

So 1058 queries cannot be categorized as either, upper, lower, or mixed case.

We can now count the number of queries composed of only upper cased characters:

\begin{lstlisting}
$ grep "^[A-Z][A-Z]*$" queries.alpha.txt | wc -l
46065
\end{lstlisting}

And the number of queries composed of only lower cased characters:

\begin{lstlisting}
$ grep "^[a-z][a-z]*$" queries.alpha.txt | wc -l
657638
\end{lstlisting}

The remaining queries are mixed case, i.e. 291538 of them.

We present the percentages relative to the total number of queries, i.e.
996299. The results are presented in \referToTable{casing}.

\item \emph{Count the number of questions (look for patterns such as starting
with Wh-words, or ending with a ``?''    symbol). What percentage of queries do
questions make up? What is the most common type of question?}

We'll first extract those queries that look like questions. One easy way of
identifying these is to see whether they start with an interrogative pronoun,
or end with a question mark.

At this point, spelling becomes important, and there are quite a few misspelled
queries. For example, there are queries that start with ``hows'' or ``how's'',
while there are other non-interrogative words that start with ``how'', e.g.
``Howard''.

We've tried to handle some of these problems by allowing a certain symbols to
follow the interrogative words, in particular (space), (comma), \texttt{s},
\texttt{?}, \texttt{:}, \texttt{+}. Otherwise, the strategy is as outlined
above.

\begin{lstlisting}
$ grep -iE "^((which|what|whose|who|whom|what|which|where|whence|
whither|when|how|why|wherefore|whether|whoever|howto|whois)[^ ',s\?:+]|.*\?)$" \
queries.txt > questions.txt
$ wc -l questions.txt 
82045 questions.txt
\end{lstlisting}

To see what we miss by using this strategy, we can first generate a file
containing queries that contain one of the interrogative words, but are not
followed by one of the above symbols:

\begin{lstlisting}
$ grep -iE "^(which|what|whose|who|whom|what|which|where|whence|
whither|when|how|why|wherefore|whether|whoever)[^ ',s\?:+][^\?]*$" \
queries.txt > badquestions.txt
$ wc -l badquestions.txt 
740 badquestions.txt
\end{lstlisting}

Observing the file, we see that we miss only a few special forms. We can refine
the method further, but this may be missing the point of this exercise.

There are 82045 queries that we've identified as queries, or 8.235\% of the
queries. We can find the most frequently asked question by sorting the
questions, counting how many times each question appears in sequence, sorting
by this count, and taking the top element.

Before we do this however, we would like to eliminate any minor variations due
to spacing and other non-alphanumeric characters:

\begin{lstlisting}
$ sed "s/[^a-zA-Z0-9]//g" questions.txt | sed "/^$/d" | \
  tr "[:upper:]" "[:lower:]" > questions.normalized.txt
$ sort questions.normalized.txt | uniq -c | sort -nr | head -1
    114 wherecanifindmathjokes
\end{lstlisting}

This does seem a little implausible. We hypothesise that if anything, this has
to do with minor spelling variations in the questions. Indeed, simply stripping
non-alphabetic characters first yields the same result.

\item \emph{What are the $k$ most common queries issued?}

\begin{lstlisting}
$ sort questions.normalized.txt | uniq -c | sort -nr | head -20
    114 wherecanifindmathjokes
     64 wherecanifindinformationoncaptech
     60 howdoesdryicework
     51 areyougay
     46 whyisdilbertsosexy
     44 whereistheisleofman
     40 howcanjeeveshelpmeshopforholidays
     36 amiinlove
     32 whyistheskyblue
     31 doesbritneyspearshavebreastimplants
     30 isjeevesgay
     29 howoldami
     29 howcanjeeveshelpmeshopfortoys
     28 whattimeisit
     27 whatisthemeaningoflife
     25 whatismyiq
     24 wherecanifindprinterdriversforahewlettpackardlaserjetiiiprinter
     23 whataretheprivateorganizationdoingtohelpalleviatepoverty
     23 howcanjeeveshelpmeshopforgifts
     21 wherecanifindsonglyrics
\end{lstlisting}

\item \emph{What percentage of queries contain stopwords like ``and'', ``the'',
``of'', `in'', ``for''?}

We will assume that the handed out list of stop words conforms to the
definition of stop words as of \cite[\textsection\ 2.2.2 (p. 27)]{irbook}, wrt.
our domain. That is, that it is a suitable subset of the words with the highest
collection frequency.

The list of stop words was retrieved from
\url{http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words} on November
27:

\begin{lstlisting}
$ md5sum stop_words.txt
f1842bdf212fb67f1f0911937310442a  stop_words.txt
\end{lstlisting}

For this question (and the next two) we will normalise our queries as follows:
we'll remove all non-alphanumeric characters, turn any sequence of spaces into
exactly one space, and lower case all the queries:

\begin{lstlisting}
$ sed "s/[^a-zA-Z0-9 ]//g" queries.txt | sed "s/  +/ /" | \
  tr "[:upper:]" "[:lower:]" > queries.normalized.txt
\end{lstlisting}

In order to use \texttt{grep} to find queries containing words in the word
list, we also need to preprocess the word list. We need to remove carriage
returns, as well as prefix and postfix every line with \texttt{\textbackslash}
indicating a word boundary. Indeed, every line should be a well-formed regular
expression.

\begin{lstlisting}
$ cat stop_words.txt | tr -d '\r' | sed 's/^\(.*\)$/\\b\1\\b/g' > stop_words.txt.ready
\end{lstlisting}

We can now proceed to use grep to count queries containing stop words:

\begin{lstlisting}
$ grep -f stop_words.txt.ready queries.normalized.txt | wc -l
274864
\end{lstlisting}

So $23.574\%$ of the queries contain stop words.

\item \emph{What are the $k$ most common non-stopwords appearing in queries
that contain the word download?}

We pick out those queries from our normalized list of queries that contain the
word download, and split those queries into words. We proceed by turning every
punctuation symbol or space into a line break, and removing those lines which
now become empty:

\begin{lstlisting}
$ grep "download" queries.normalized.txt | tr "[[:punct:][:space:]]" "\n" | \
  sed "/^$/d" > words.txt
\end{lstlisting}

We can now proceed to pick out those words that do not occur in our
preprocessed stopwords file. We then sort those words, pick count their unique
occurrences, sort by the counts, and take the top 20:

\begin{lstlisting}
$ grep -vf stop_words.txt.ready words.txt | sort | uniq -c | sort -nr | head -20
   3441 download
   1110 downloads
    837 free
    394 games
    282 mp3
    256 game
    237 music
    133 downloadable
    109 software
    108 pokemon
     92 windows
     78 christmas
     70 mac
     68 player
     67 pc
     66 internet
     65 demo
     64 funny
     60 files
     55 cd
\end{lstlisting}

As might be expected, people are looking for free downloads of games, music
(mp3), and software. 

\item \emph{What are the $k$ most common non-stopwords appearing in queries?}

We'll proceed as above, except that instead of picking out those queries where
the word ``download'' appears, we'll simply \texttt{cat} the normalized
queries.

\begin{lstlisting}
$ cat queries.normalized.txt | tr "[[:punct:][:space:]]" "\n" | sed "/^$/d" \
  > words.txt
$ grep -vf stop_words.txt.ready words.txt | sort | uniq -c | sort -nr | head -20
  20934 free
  13306 pictures
  13117 information
  12382 christmas
  11500 sex
   9369 nude
   8279 new
   7108 buy
   6775 pics
   6708 online
   5670 web
   5501 music
   5232 games
   4687 women
   4272 does
   4177 stories
   4126 site
   4021 cards
   3966 porn
   3850 state
\end{lstlisting}

\item \emph{What percentage of queries were asked by only one user?}

We can compute the number of queries executed by a single user by extracting
column 2 from our \texttt{excite-999000.no3.by.2-1.uniq.txt} constructed
earlier:

\begin{lstlisting}
$ cut -f2 excite-999000.no3.by.2-1.uniq.txt | uniq -u | wc -l
100439
\end{lstlisting}

This is $10.081\%$ of all queries.

\item \emph{How often do URLs appear in queries?}

URLs can get fairly complex, especially when non-latin characters are allowed.
We'll find a subset of queries that look like they contain URLs in a very
simple way: if they contain a sequence of characters followed by a dot,
followed by a sequence of alphabetic characters (the top-level domain) of
length at least two:

\begin{lstlisting}
$ grep -iE "[^@ ]+\.[a-z]{2,}" queries.txt | wc -l
46272
\end{lstlisting}

A quick look at the queries that this yields tells we are also getting e.g.
\texttt{.exe} files.

% We do not find that queries themselves may put anyone's piracy at risk, it's
% how we chose to answer them that might.

% There are two ways in which this can happen. One is that the user targets a
% particular individual, looking for things that would otherwise be considered
% private, e.g. nude pictures. The other is that the user targets the software
% that handles the query, i.e. attempts to perform a computer attack.

% We make this distinction as the style of the queries is radically different.

% We'll take a look at the first class of queries first. Some goto examples
% reveal a couple malicious queries related to female names:

% \begin{lstlisting}
% $ grep -iE "(nude|naked|sexy).*britney|britney.*(nude|naked|sexy)" \
%  queries.txt | wc -l
% 564
% $ grep -iE "(nude|naked|sexy).*jenn|jenn.*(nude|naked|sexy)" \
%   queries.txt | wc -l
% 356
% \end{lstlisting}

\item \emph{Can you find addresses phone numbers, or other identifiers in the
log file?}

Phone numbers and addresses tend to be locale-dependent. It is unclear which
geographical region we're dealing with in general, so we'll try to stay locale
independent.

First, we normalize the queries in the sense that we remove things that look
like years or year ranges:

\begin{lstlisting}
$ sed "/\(^\| \)[0-9]\{4,4\}\( *- *[0-9]\{4,4\}\)\?\( \|$\)/d" \
  queries.txt > queries.no.years.txt
\end{lstlisting}

Then we look for a sequence of at least 5 numbers, spaces, or dashes.

\begin{lstlisting}
$ grep -iE "(^| )[0-9 -]{5,}( |$)" queries.no.years.txt > phone.numbers.txt
\end{lstlisting}

This strategy yields some telephone numbers e.g.

\begin{lstlisting}
107-41-50
1-800-285-5100
1 800 flowers
(603) 627-4613
Brandon Siljord, 400 Pine Ave., Little Falls, Mn, 56345  , 320-632-8172
\end{lstlisting}

However, we also get a lot of garbage. The problem seems to be the myriad of
locale variations and spelling mistakes.

\item \emph{Is query volume constant throughout the day?}

We extract the leading 3 characters of every line, sort the numbers, and count
unique consecutive lines:

\begin{lstlisting}
$ cut -c-3 excite-999000.no3.by.2-1.uniq.txt | sort -n | uniq -c
  60784 090
  61473 091
  58620 092
  54284 093
  55474 094
  55204 095
  54038 100
  55523 101
  56564 102
  57869 103
  56792 104
  55792 105
  55881 110
  56722 111
  55923 112
  54744 113
  55517 114
  35095 115
\end{lstlisting}

To get the times of the first and last queries, we do something similar, but
extract the 6 leading characters, and use \texttt{head} and \texttt{tail} to
find the earliest and latest query time, respectively:

\begin{lstlisting}
$ cut -c-6 excite-999000.no3.by.2-1.uniq.txt | sort -n | uniq | head -1
090000
$ cut -c-6 excite-999000.no3.by.2-1.uniq.txt | sort -n | uniq | tail -1
115618
\end{lstlisting}

\end{itemize}

